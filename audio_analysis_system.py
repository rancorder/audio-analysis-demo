"""
éŸ³å£°è§£æçµ±åˆã‚·ã‚¹ãƒ†ãƒ  - ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰
éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´æŠ½å‡º + æ–‡å­—èµ·ã“ã— + æ„Ÿæƒ…åˆ†æã‚’çµ±åˆ

ä½¿ç”¨æŠ€è¡“:
- Whisper: æ–‡å­—èµ·ã“ã—
- OpenSMILE: éŸ³å£°ç‰¹å¾´æŠ½å‡º
- BERT: æ„Ÿæƒ…åˆ†æ
- librosa, noisereduce: å‰å‡¦ç†
"""

import os
import torch
import whisper
import opensmile
import librosa
import soundfile as sf
import tkinter as tk
from tkinter import filedialog
from pydub import AudioSegment
import multiprocessing
import logging
import subprocess
import sys
import noisereduce as nr
import pandas as pd
from transformers import pipeline

# ===========================
# ãƒ­ã‚°è¨­å®š
# ===========================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)

# ===========================
# CPUæœ€é©åŒ–
# ===========================
torch.set_num_threads(min(2, os.cpu_count()))

# ===========================
# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
# ===========================
WHISPER_MODEL = whisper.load_model("tiny")
OPENSMILE = opensmile.Smile(
    feature_set=opensmile.FeatureSet.eGeMAPSv02,
    feature_level=opensmile.FeatureLevel.Functionals,
)
SENTIMENT_ANALYZER = pipeline(
    "text-classification",
    model="nlptown/bert-base-multilingual-uncased-sentiment"
)

# ===========================
# ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
# ===========================
def select_audio_file():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã•ã›ã‚‹"""
    root = tk.Tk()
    root.withdraw()
    file_path = filedialog.askopenfilename(
        filetypes=[("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«", "*.wav *.mp3 *.m4a")]
    )
    if not file_path:
        logging.warning("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        exit(1)
    logging.info(f"ğŸ“‚ é¸æŠ: {file_path}")
    return file_path

# ===========================
# éŸ³å£°å‰å‡¦ç†
# ===========================
def convert_to_wav(input_path, output_path):
    """MP3/M4Aã‚’WAVã«å¤‰æ›ï¼ˆFFmpegä½¿ç”¨ï¼‰"""
    try:
        logging.info("ğŸ”„ WAVå¤‰æ›ä¸­...")
        subprocess.run(
            ["ffmpeg", "-i", input_path, "-ar", "16000", "-ac", "1", 
             "-b:a", "32k", output_path, "-y"],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        logging.info("âœ… å¤‰æ›å®Œäº†")
        return output_path
    except subprocess.CalledProcessError as e:
        logging.error(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        exit(1)

def denoise_audio(input_path, output_path):
    """ãƒã‚¤ã‚ºé™¤å»å‡¦ç†"""
    try:
        # MP3ã®å ´åˆã¯å¤‰æ›
        if input_path.lower().endswith(".mp3"):
            input_path = convert_to_wav(input_path, "temp_converted.wav")
        
        logging.info("ğŸ”Š ãƒã‚¤ã‚ºé™¤å»ä¸­...")
        audio, sr = librosa.load(input_path, sr=16000)
        reduced_noise = nr.reduce_noise(y=audio, sr=sr, prop_decrease=0.8)
        sf.write(output_path, reduced_noise, sr)
        logging.info("âœ… ãƒã‚¤ã‚ºé™¤å»å®Œäº†")
        return output_path
    except Exception as e:
        logging.error(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
        return convert_to_wav(input_path, output_path)

def split_audio(file_path, chunk_length_ms=15000):
    """éŸ³å£°ã‚’15ç§’å˜ä½ã§åˆ†å‰²"""
    logging.info("âœ‚ï¸ éŸ³å£°åˆ†å‰²ä¸­...")
    audio = AudioSegment.from_file(file_path)
    chunks = [
        audio[i:i+chunk_length_ms] 
        for i in range(0, len(audio), chunk_length_ms)
    ]
    
    chunk_files = []
    for i, chunk in enumerate(chunks):
        chunk_path = f"chunk_{i}.wav"
        chunk.export(chunk_path, format="wav")
        chunk_files.append(chunk_path)
    
    logging.info(f"âœ… {len(chunk_files)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
    return chunk_files

# ===========================
# Whisperæ–‡å­—èµ·ã“ã—
# ===========================
def transcribe_audio(file_path):
    """Whisperã§éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—"""
    try:
        logging.info(f"ğŸ™ï¸ æ–‡å­—èµ·ã“ã—ä¸­: {file_path}")
        result = WHISPER_MODEL.transcribe(
            file_path,
            fp16=True,
            language="ja",
            temperature=0
        )
        return result["text"]
    except Exception as e:
        logging.error(f"âŒ æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
        return ""

def parallel_transcription(files):
    """ä¸¦åˆ—å‡¦ç†ã§è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—"""
    num_processes = 2
    logging.info(f"âš¡ ä¸¦åˆ—å‡¦ç†é–‹å§‹ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ•°: {num_processes}ï¼‰")
    
    with multiprocessing.Pool(processes=num_processes) as pool:
        results = pool.map(transcribe_audio, files, chunksize=2)
        pool.close()
        pool.join()
    
    return results

# ===========================
# OpenSMILEéŸ³å£°ç‰¹å¾´æŠ½å‡º
# ===========================
def extract_audio_features(audio_path):
    """OpenSMILEã§éŸ³å£°ç‰¹å¾´ã‚’æŠ½å‡º"""
    logging.info("ğŸ“Š éŸ³å£°ç‰¹å¾´æŠ½å‡ºä¸­...")
    
    # å…¨ç‰¹å¾´é‡ã‚’æŠ½å‡º
    features = OPENSMILE.process_file(audio_path)
    
    # é‡è¦ãªç‰¹å¾´é‡ã‚’é¸æŠ
    selected_features = [
        "F0semitoneFrom27.5Hz_sma3nz_amean",      # ãƒ”ãƒƒãƒå¹³å‡
        "F0semitoneFrom27.5Hz_sma3nz_stddevNorm", # ãƒ”ãƒƒãƒã°ã‚‰ã¤ã
        "shimmerLocaldB_sma3nz_amean",            # ã‚·ãƒãƒ¼
        "jitterLocal_sma3nz_amean",               # ã‚¸ãƒƒã‚¿ãƒ¼
        "loudness_sma3_amean",                    # éŸ³é‡å¹³å‡
        "loudness_sma3_stddevNorm"                # éŸ³é‡ã°ã‚‰ã¤ã
    ]
    
    # å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    missing = [f for f in selected_features if f not in features.columns]
    if missing:
        logging.warning(f"âš ï¸ æœªå–å¾—ã®ç‰¹å¾´é‡: {missing}")
    
    available_features = [f for f in selected_features if f in features.columns]
    result = features[available_features]
    
    logging.info("âœ… éŸ³å£°ç‰¹å¾´æŠ½å‡ºå®Œäº†")
    return result

# ===========================
# BERTæ„Ÿæƒ…åˆ†æ
# ===========================
def analyze_sentiment(text):
    """BERTã§æ„Ÿæƒ…åˆ†æ"""
    logging.info("ğŸ§  æ„Ÿæƒ…åˆ†æä¸­...")
    
    if not text.strip():
        logging.warning("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
        return 3  # ä¸­ç«‹
    
    result = SENTIMENT_ANALYZER(text[:512])  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™
    emotion_score = int(result[0]["label"][0])  # "4 stars" â†’ 4
    
    logging.info(f"âœ… æ„Ÿæƒ…ã‚¹ã‚³ã‚¢: {emotion_score}/5")
    return emotion_score

# ===========================
# Late Fusionï¼ˆç‰¹å¾´çµ±åˆï¼‰
# ===========================
def late_fusion(audio_features, sentiment_score, transcription):
    """éŸ³å£°ç‰¹å¾´ã¨ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã‚’çµ±åˆ"""
    logging.info("ğŸ”— ç‰¹å¾´çµ±åˆä¸­...")
    
    combined = {
        "audio_features": audio_features.to_dict('records')[0],
        "sentiment_score": sentiment_score,
        "transcription": transcription,
        "feature_count": len(audio_features.columns)
    }
    
    logging.info("âœ… ç‰¹å¾´çµ±åˆå®Œäº†")
    return combined

# ===========================
# çµæœä¿å­˜
# ===========================
def save_results(combined_features, output_file="analysis_result.txt"):
    """åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("=== éŸ³å£°è§£æçµæœ ===\n\n")
        
        f.write("ã€éŸ³å£°ç‰¹å¾´ã€‘\n")
        for key, value in combined_features["audio_features"].items():
            f.write(f"  {key}: {value}\n")
        
        f.write(f"\nã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã€‘\n")
        f.write(f"  {combined_features['sentiment_score']}/5\n")
        
        f.write(f"\nã€æ–‡å­—èµ·ã“ã—ã€‘\n")
        f.write(f"  {combined_features['transcription']}\n")
    
    logging.info(f"âœ… çµæœä¿å­˜: {output_file}")

# ===========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===========================
def main():
    """çµ±åˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼"""
    logging.info("ğŸ”· éŸ³å£°è§£æçµ±åˆã‚·ã‚¹ãƒ†ãƒ  èµ·å‹•")
    
    # 1. ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
    audio_path = select_audio_file()
    
    # 2. å‰å‡¦ç†ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
    denoised_path = "denoised_audio.wav"
    denoised_path = denoise_audio(audio_path, denoised_path)
    
    # 3. éŸ³å£°åˆ†å‰²
    chunk_files = split_audio(denoised_path)
    
    # 4. ä¸¦åˆ—æ–‡å­—èµ·ã“ã—
    transcriptions = parallel_transcription(chunk_files)
    full_transcription = "\n".join(transcriptions)
    
    # 5. éŸ³å£°ç‰¹å¾´æŠ½å‡º
    audio_features = extract_audio_features(denoised_path)
    
    # 6. æ„Ÿæƒ…åˆ†æ
    sentiment_score = analyze_sentiment(full_transcription)
    
    # 7. Late Fusion
    combined = late_fusion(audio_features, sentiment_score, full_transcription)
    
    # 8. çµæœä¿å­˜
    save_results(combined)
    
    # 9. ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    for chunk_file in chunk_files:
        if os.path.exists(chunk_file):
            os.remove(chunk_file)
    
    logging.info("âœ… å…¨å‡¦ç†å®Œäº†ï¼")
    
    return combined

if __name__ == "__main__":
    results = main()
