"""
æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰
å¤šå‡ºåŠ›å›å¸°å•é¡Œã®å®Ÿè£…ä¾‹ï¼ˆXGBoost + RandomForestï¼‰

ç‰¹å¾´:
- çŸ­æœŸãƒ‡ãƒ¼ã‚¿ + é•·æœŸãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
- 6ã¤ã®ç‹¬ç«‹ã—ãŸäºˆæ¸¬å€¤ã‚’åŒæ™‚æ¨å®š
- ãƒã‚¤ã‚ºä»˜åŠ ã«ã‚ˆã‚‹ãƒ­ãƒã‚¹ãƒˆæ€§å‘ä¸Š
- é©åˆ‡ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
"""

import numpy as np
import pandas as pd
import tkinter as tk
from tkinter import filedialog
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from collections import Counter
import datetime
import os

# ===========================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ===========================
def load_data():
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã§é¸æŠã—ã¦èª­ã¿è¾¼ã¿"""
    root = tk.Tk()
    root.withdraw()
    file_path = filedialog.askopenfilename(
        title="CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
        filetypes=[("CSV files", "*.csv")]
    )
    
    if not file_path:
        raise ValueError("ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    df = pd.read_csv(file_path, encoding="utf-8")
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")
    
    return df, file_path

# ===========================
# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
# ===========================
def create_features(data, N=5, max_past_data=500):
    """
    çŸ­æœŸãƒ‡ãƒ¼ã‚¿ + é•·æœŸãƒ‡ãƒ¼ã‚¿ã®çµ±åˆç‰¹å¾´é‡ã‚’ä½œæˆ
    
    Args:
        data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿é…åˆ—
        N: ç›´è¿‘Nå›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        max_past_data: çŸ­æœŸãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§é•·
    
    Returns:
        X_recent: ç›´è¿‘ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
        X_past: éå»ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
        y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
    """
    X_recent, X_past, y = [], [], []
    full_data_length = len(data)
    
    for i in range(N, full_data_length):
        # 1. ç›´è¿‘Nå›ã®ãƒ‡ãƒ¼ã‚¿
        recent_data = data[i-N:i].flatten()
        X_recent.append(recent_data)
        
        # 2. ã™ã¹ã¦ã®éå»ãƒ‡ãƒ¼ã‚¿
        all_past = data[:i].flatten()
        
        # 3. çŸ­æœŸãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€æ–°500ä»¶ï¼‰
        if len(all_past) > max_past_data:
            past_500 = all_past[-max_past_data:]
        else:
            # ä¸è¶³åˆ†ã‚’ã‚¼ãƒ­åŸ‹ã‚
            pad_length = max_past_data - len(all_past)
            past_500 = np.pad(all_past, (pad_length, 0), mode='constant')
        
        # 4. é•·æœŸãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’åˆ¶é™
        all_past_limited = all_past[:full_data_length]
        
        # 5. ç‰¹å¾´é‡ã‚’çµåˆ
        combined_features = np.hstack([past_500, all_past_limited])
        
        # 6. é•·ã•ã‚’çµ±ä¸€ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
        target_length = max_past_data + full_data_length
        if len(combined_features) < target_length:
            pad_length = target_length - len(combined_features)
            combined_features = np.pad(
                combined_features, 
                (0, pad_length), 
                mode='constant'
            )
        
        X_past.append(combined_features)
        
        # 7. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ¬¡å›ã®å€¤ã‚»ãƒƒãƒˆï¼‰
        y.append(data[i])
    
    return np.array(X_recent), np.array(X_past), np.array(y)

# ===========================
# ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆåˆ†æ
# ===========================
def analyze_data_statistics(data, column_names):
    """ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã‚’åˆ†æ"""
    all_numbers = data.flatten()
    total_count = len(all_numbers)
    number_counts = Counter(all_numbers)
    
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆï¼ˆç·ãƒ‡ãƒ¼ã‚¿æ•°: {total_count}ï¼‰")
    
    # é »å‡ºTOP10
    most_common = number_counts.most_common(10)
    print("\né »å‡ºTOP10:")
    for num, count in most_common:
        percentage = (count / total_count) * 100
        print(f"  {num}: {count}å› ({percentage:.2f}%)")
    
    # æœ€ã‚‚å‡ºã¦ã„ãªã„TOP10
    all_possible = set(range(1, 44))
    missing_counts = {num: number_counts.get(num, 0) for num in all_possible}
    least_common = sorted(missing_counts.items(), key=lambda x: x[1])[:10]
    
    print("\nå‡ºç¾ãŒå°‘ãªã„TOP10:")
    for num, count in least_common:
        percentage = (count / total_count) * 100 if total_count > 0 else 0
        print(f"  {num}: {count}å› ({percentage:.2f}%)")
    
    return number_counts

# ===========================
# ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
# ===========================
def encode_labels(y_train, y_test, n_outputs=6):
    """
    å„å‡ºåŠ›ã”ã¨ã«ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    
    Args:
        y_train: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        y_test: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        n_outputs: å‡ºåŠ›æ•°
    
    Returns:
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒªã‚¹ãƒˆ
    """
    label_encoders = []
    y_train_encoded = []
    y_test_encoded = []
    
    for i in range(n_outputs):
        # å…¨ã‚¯ãƒ©ã‚¹ã‚’å–å¾—
        all_classes = np.unique(
            np.concatenate([y_train[:, i], y_test[:, i]])
        )
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        encoder = LabelEncoder()
        encoder.fit(all_classes)
        label_encoders.append(encoder)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
        y_train_encoded.append(encoder.transform(y_train[:, i]))
        y_test_encoded.append(encoder.transform(y_test[:, i]))
    
    y_train_encoded = np.column_stack(y_train_encoded)
    y_test_encoded = np.column_stack(y_test_encoded)
    
    print(f"âœ… ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†ï¼ˆ{n_outputs}å‡ºåŠ›ï¼‰")
    
    return y_train_encoded, y_test_encoded, label_encoders

# ===========================
# XGBoostãƒ¢ãƒ‡ãƒ«è¨“ç·´
# ===========================
def train_xgboost_models(X_train, y_train_encoded, n_outputs=6):
    """
    å„å‡ºåŠ›ã”ã¨ã«XGBoostãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    
    Args:
        X_train: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        y_train_encoded: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        n_outputs: å‡ºåŠ›æ•°
    
    Returns:
        è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    print("\nğŸ¯ XGBoostãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
    models = []
    
    for i in range(n_outputs):
        print(f"  ãƒ¢ãƒ‡ãƒ« {i+1}/{n_outputs} è¨“ç·´ä¸­...")
        
        xgb = XGBClassifier(
            n_estimators=50,
            learning_rate=0.05,
            max_depth=3,
            subsample=0.6,
            eval_metric='mlogloss',
            random_state=42
        )
        
        xgb.fit(X_train, y_train_encoded[:, i])
        models.append(xgb)
    
    print("âœ… XGBoostãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
    return models

# ===========================
# RandomForestãƒ¢ãƒ‡ãƒ«è¨“ç·´
# ===========================
def train_randomforest_models(X_train, y_train_encoded, n_outputs=6):
    """
    å„å‡ºåŠ›ã”ã¨ã«RandomForestãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    
    Args:
        X_train: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        y_train_encoded: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        n_outputs: å‡ºåŠ›æ•°
    
    Returns:
        è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    print("\nğŸŒ² RandomForestãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
    models = []
    
    for i in range(n_outputs):
        print(f"  ãƒ¢ãƒ‡ãƒ« {i+1}/{n_outputs} è¨“ç·´ä¸­...")
        
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=7,
            min_samples_split=4,
            min_samples_leaf=2,
            random_state=42
        )
        
        rf.fit(X_train, y_train_encoded[:, i])
        models.append(rf)
    
    print("âœ… RandomForestãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
    return models

# ===========================
# äºˆæ¸¬å®Ÿè¡Œï¼ˆãƒã‚¤ã‚ºä»˜åŠ ï¼‰
# ===========================
def predict_with_noise(models, X_latest, label_encoders, 
                       noise_range=(-3, 4), clip_range=(1, 43)):
    """
    ãƒã‚¤ã‚ºã‚’ä»˜åŠ ã—ã¦ãƒ­ãƒã‚¹ãƒˆãªäºˆæ¸¬ã‚’å®Ÿè¡Œ
    
    Args:
        models: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        X_latest: æœ€æ–°ãƒ‡ãƒ¼ã‚¿
        label_encoders: ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒªã‚¹ãƒˆ
        noise_range: ãƒã‚¤ã‚ºã®ç¯„å›²
        clip_range: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®ç¯„å›²
    
    Returns:
        äºˆæ¸¬å€¤ã®é…åˆ—
    """
    # ãƒã‚¤ã‚ºä»˜åŠ 
    noise = np.random.randint(*noise_range, size=X_latest.shape)
    X_with_noise = np.clip(X_latest + noise, *clip_range)
    
    # å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
    predictions = []
    for i, model in enumerate(models):
        pred_encoded = model.predict(X_with_noise)
        pred_decoded = label_encoders[i].inverse_transform(pred_encoded)
        predictions.append(pred_decoded)
    
    result = np.column_stack(predictions)
    return np.sort(result.flatten())

# ===========================
# çµæœä¿å­˜
# ===========================
def save_results(xgb_pred, rf_pred, stats, output_dir):
    """äºˆæ¸¬çµæœã¨ãƒ­ã‚°ã‚’ä¿å­˜"""
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_file = os.path.join(output_dir, f"prediction_log_{timestamp}.txt")
    
    with open(log_file, "w", encoding="utf-8") as f:
        f.write("=== æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬çµæœ ===\n\n")
        
        f.write("ã€XGBoostäºˆæ¸¬ã€‘\n")
        f.write(f"  {xgb_pred.tolist()}\n\n")
        
        f.write("ã€RandomForestäºˆæ¸¬ã€‘\n")
        f.write(f"  {rf_pred.tolist()}\n\n")
        
        f.write("ã€ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã€‘\n")
        f.write(f"  ç·ãƒ‡ãƒ¼ã‚¿æ•°: {sum(stats.values())}\n")
        f.write(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°: {len(stats)}\n")
    
    print(f"âœ… çµæœä¿å­˜: {log_file}")
    return log_file

# ===========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===========================
def main():
    """çµ±åˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼"""
    print("ğŸ”· æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  èµ·å‹•\n")
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df, file_path = load_data()
    output_dir = os.path.dirname(file_path)
    
    # æƒ³å®š: 6åˆ—ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿
    number_columns = ['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    data = df[number_columns].values
    
    # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    assert data.shape[1] == 6, "ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã¯6åˆ—å¿…è¦ã§ã™"
    
    # 2. ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆåˆ†æ
    stats = analyze_data_statistics(data, number_columns)
    
    # 3. ç‰¹å¾´é‡ä½œæˆ
    print("\nâš™ï¸ ç‰¹å¾´é‡ä½œæˆä¸­...")
    X_recent, X_past, y = create_features(data, N=5, max_past_data=500)
    
    # ç‰¹å¾´é‡çµåˆ
    X_combined = np.hstack([X_recent, X_past])
    print(f"âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†: {X_combined.shape}")
    
    # 4. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_combined, y, test_size=0.2, random_state=42
    )
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: è¨“ç·´{len(X_train)}ä»¶, ãƒ†ã‚¹ãƒˆ{len(X_test)}ä»¶")
    
    # 5. ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    y_train_enc, y_test_enc, encoders = encode_labels(y_train, y_test)
    
    # 6. XGBoostãƒ¢ãƒ‡ãƒ«è¨“ç·´
    xgb_models = train_xgboost_models(X_train, y_train_enc)
    
    # 7. RandomForestãƒ¢ãƒ‡ãƒ«è¨“ç·´
    rf_models = train_randomforest_models(X_train, y_train_enc)
    
    # 8. æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äºˆæ¸¬
    print("\nğŸ¯ äºˆæ¸¬å®Ÿè¡Œä¸­...")
    num_recent = 20
    recent_samples = X_combined[-num_recent:].mean(axis=0).reshape(1, -1)
    
    xgb_pred = predict_with_noise(xgb_models, recent_samples, encoders)
    rf_pred = predict_with_noise(rf_models, recent_samples, encoders)
    
    print(f"\nğŸ¯ XGBoostäºˆæ¸¬: {xgb_pred.tolist()}")
    print(f"ğŸ¯ RandomForestäºˆæ¸¬: {rf_pred.tolist()}")
    
    # 9. çµæœä¿å­˜
    save_results(xgb_pred, rf_pred, stats, output_dir)
    
    print("\nâœ… å…¨å‡¦ç†å®Œäº†ï¼")
    
    return {
        'xgb_prediction': xgb_pred,
        'rf_prediction': rf_pred,
        'statistics': stats
    }

if __name__ == "__main__":
    results = main()